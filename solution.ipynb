{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d13361-7b0e-415f-8121-e503b32884fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.sparse import coo_matrix, save_npz, load_npz\n",
    "import implicit\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a806c440-4fda-4ee9-80b7-e0d5e8274bfb",
   "metadata": {},
   "source": [
    "**Вспомогательные функции**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a98d37c-4437-41ec-97a0-87bc4911a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read table from SSD/HDD\n",
    "def read_df_from_parquets(path_to_parquets: str, n=None):\n",
    "    files = glob.glob(path_to_parquets + '/*.parquet')\n",
    "    count_to_read = len(files) if n is None else min(n, len(files))\n",
    "    return pl.read_parquet(files[:count_to_read])\n",
    "\n",
    "# scan table from SSD/HDD\n",
    "def scan_df_from_parquets(path_to_parquets: str, n=None):\n",
    "    files = glob.glob(path_to_parquets + '/*.parquet')\n",
    "    count_to_scan = len(files) if n is None else min(n, len(files))\n",
    "    return pl.scan_parquet(files[:count_to_scan])\n",
    "\n",
    "# write table to SSD/HDD with chunks\n",
    "def write_parquet_in_chunks(df: pl.DataFrame, path: str, chunk_size=100000):\n",
    "    out = Path(path)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    for i in range(0, df.height, chunk_size):\n",
    "        chunk = df.slice(i, chunk_size)\n",
    "        file_path = out / f\"part_{i // chunk_size:04d}.parquet\"\n",
    "        chunk.write_parquet(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2e3b8a-54d8-4f32-9626-18a0cf4075ac",
   "metadata": {},
   "source": [
    "### 1. Обработка таблицы логов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2b584-9f7d-45e3-8262-bc82aef97234",
   "metadata": {},
   "source": [
    "Сначала внутри каждого .parquet файла происходит агрегация и создается сгрупированный файл .parquet в папку agg_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d56320e4-d43e-4dcb-b642-a4adb96af6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_logs_table_one_by_one(\n",
    "    path_to_orders: str,\n",
    "    path_to_tracker: str,\n",
    "    output_path='./data/agg_interactions',\n",
    "):\n",
    "    agg_interactions_dir = Path(output_path)\n",
    "    agg_interactions_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    df_orders = (\n",
    "         scan_df_from_parquets(path_to_orders)\n",
    "         .select([\"user_id\", \"item_id\", \"last_status\"])\n",
    "         .with_columns([\n",
    "               pl.col(\"user_id\").cast(pl.Int64),\n",
    "                pl.col(\"item_id\").cast(pl.Int64),\n",
    "                pl.col(\"last_status\")\n",
    "         ])\n",
    "         .filter(pl.col(\"last_status\") != \"proccesed_orders\")\n",
    "    )\n",
    "    \n",
    "    def encode_actions(df: pl.LazyFrame):\n",
    "        return df.with_columns([\n",
    "            (pl.col(\"action_type\") == \"view_description\").cast(pl.Int32).alias(\"action_type_view_description\"),\n",
    "            (pl.col(\"action_type\") == \"to_cart\").cast(pl.Int32).alias(\"action_type_to_cart\"),\n",
    "            (pl.col(\"action_type\") == \"page_view\").cast(pl.Int32).alias(\"action_type_page_view\"),\n",
    "            (pl.col(\"action_type\") == \"favorite\").cast(pl.Int32).alias(\"action_type_favorite\"),\n",
    "            (pl.col(\"action_type\") == \"unfavorite\").cast(pl.Int32).alias(\"action_type_unfavorite\"),\n",
    "            (pl.col(\"action_type\") == \"review_view\").cast(pl.Int32).alias(\"action_type_review_view\"),\n",
    "            (pl.col(\"action_type\") == \"remove\").cast(pl.Int32).alias(\"action_type_remove\"),\n",
    "            pl.when(pl.col(\"last_status\") == \"canceled_orders\").then(0)\n",
    "              .when(pl.col(\"last_status\") == \"delivered_orders\").then(1)\n",
    "              .otherwise(0)\n",
    "              .cast(pl.Int8)\n",
    "              .alias(\"last_status\")\n",
    "        ]).drop(\"action_type\")\n",
    "\n",
    "    tracker_files = sorted(glob.glob(path_to_tracker))\n",
    "    chunk_size = 1\n",
    "    for i in range(0, len(tracker_files), chunk_size):\n",
    "        files_chunk = tracker_files[i:i+chunk_size]\n",
    "        print(f\"Processing chunk {i//chunk_size + 1}/{(len(tracker_files)+chunk_size-1)//chunk_size}\")\n",
    "    \n",
    "        chunk_df = (pl.scan_parquet(files_chunk)\n",
    "                    .with_columns([\n",
    "                        pl.col(\"user_id\").cast(pl.Int64),\n",
    "                        pl.col(\"item_id\").cast(pl.Int64),\n",
    "                    ]))\n",
    "    \n",
    "        temp = (\n",
    "            chunk_df\n",
    "            .join(df_orders, on=[\"item_id\",\"user_id\"], how=\"left\")\n",
    "        )\n",
    "        temp = encode_actions(temp)\n",
    "    \n",
    "        temp_agg = temp.group_by([\"user_id\",\"item_id\"]).agg([\n",
    "            pl.col([\n",
    "                \"action_type_view_description\",\n",
    "                \"action_type_to_cart\",\n",
    "                \"action_type_page_view\",\n",
    "                \"action_type_favorite\",\n",
    "                \"action_type_unfavorite\",\n",
    "                \"action_type_review_view\",\n",
    "                \"action_type_remove\"\n",
    "            ]).sum(),\n",
    "            pl.col(\"last_status\").first()\n",
    "        ])\n",
    "    \n",
    "        output_file = agg_interactions_dir / f\"agg_chunk_{i//chunk_size}.parquet\"\n",
    "        temp_agg.collect(engine='streaming').write_parquet(output_file)\n",
    "        print(f\"Saved {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1a92c6e-20e7-406a-a4be-e5fa16721c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_logs_table(\n",
    "    agg_interactions_path='./data/agg_interactions_dir',\n",
    "    interactions_path='./data/interactions'\n",
    "):\n",
    "    chunks_dir = Path(agg_interactions_path)\n",
    "\n",
    "    all_files = sorted(glob.glob(str(chunks_dir / \"*.parquet\")))\n",
    "    \n",
    "    out_path = Path(interactions_path)\n",
    "    out_path.mkdir(exist_ok=True)\n",
    "    chunk_size = 2\n",
    "    \n",
    "    for i in tqdm(range(0, len(all_files), chunk_size)):\n",
    "        batch = all_files[i:i+chunk_size]\n",
    "        \n",
    "        # читаем сразу все parquet из батча\n",
    "        df = pl.scan_parquet(batch)\n",
    "        agg_data = (\n",
    "            df\n",
    "            .group_by([\"user_id\",\"item_id\"])\n",
    "            .agg([\n",
    "                pl.col([\n",
    "                    \"action_type_view_description\",\n",
    "                    \"action_type_to_cart\",\n",
    "                    \"action_type_page_view\",\n",
    "                    \"action_type_favorite\",\n",
    "                    \"action_type_unfavorite\",\n",
    "                    \"action_type_review_view\",\n",
    "                    \"action_type_remove\"\n",
    "                ]).sum(),\n",
    "                pl.col(\"last_status\").first()\n",
    "            ])\n",
    "        )\n",
    "        del df\n",
    "        out_file = out_path / f\"grouped_batch_{i//chunk_size + 1}.parquet\"\n",
    "        agg_data.sink_parquet(out_file)\n",
    "        del agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34685be2-f653-48da-b430-a476088c51f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/1\n"
     ]
    }
   ],
   "source": [
    "def analyze_logs_table(\n",
    "    path_to_orders='./data/ml_ozon_recsys_train_final_apparel_orders_data',\n",
    "    path_to_tracker='./data/ml_ozon_recsys_train_final_apparel_tracker_data',\n",
    "    agg_interactions_path='./data/agg_interactions',\n",
    "    interactions_path='./data/interactions'\n",
    "):\n",
    "    analyze_logs_table_one_by_one(path_to_orders, path_to_tracker, agg_interactions_path)\n",
    "    compress_logs_table(agg_interactions_path, interactions_path)\n",
    "\n",
    "analyze_logs_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d83a8-6f8c-4c03-bc5c-f7894643a252",
   "metadata": {},
   "source": [
    "#### 2. Нахождение коэффициентов для рейтинга с помощью логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097b8cc0-3331-4377-b1b1-1217ccc170b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_type_view_description': -0.09608848421927428, 'action_type_to_cart': 2.4621577568713238, 'action_type_page_view': 0.1828402680655119, 'action_type_favorite': -0.22149147361203042, 'action_type_unfavorite': 0.18611649944352027, 'action_type_review_view': 0.026484669994032747, 'action_type_remove': -1.9714975325140849}\n"
     ]
    }
   ],
   "source": [
    "def find_weights_for_rating(\n",
    "    feature_cols: list,\n",
    "    interactions_path='./data/interactions',\n",
    "):\n",
    "    interactions = read_df_from_parquets(interactions_path)\n",
    "    \n",
    "    X_train = interactions.select(feature_cols).to_numpy()\n",
    "    y_train = interactions['last_status'].to_numpy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model = model.fit(X_scaled, y_train)\n",
    "\n",
    "    coefs = model.coef_.ravel()\n",
    "\n",
    "    weights = dict(zip(feature_cols, coefs))\n",
    "    return weights\n",
    "\n",
    "feature_cols = [\n",
    "    \"action_type_view_description\",\n",
    "    \"action_type_to_cart\",\n",
    "    \"action_type_page_view\",\n",
    "    \"action_type_favorite\",\n",
    "    \"action_type_unfavorite\",\n",
    "    \"action_type_review_view\",\n",
    "    \"action_type_remove\"\n",
    "]\n",
    "\n",
    "weights = find_weights_for_rating(feature_cols)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75198a37-860b-470d-9fa3-fae7bb89ec31",
   "metadata": {},
   "source": [
    "#### 3. Генерируем матрицу R для ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b52ef-113c-4bad-a9b6-643f0b1d319b",
   "metadata": {},
   "source": [
    "На пересечении R[i, j] лежит линейная комбинация действий пользователя i с товаром j с коэффициентами, полученными в прошлом пункте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2481682-8821-41e2-92e1-8102b6572ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_matrix_R(\n",
    "    train_data_path,\n",
    "    output_path,\n",
    "    weights,\n",
    "    n_parquets=10\n",
    "):\n",
    "    all_files = sorted(glob.glob(str(train_data_path /\"*.parquet\")))\n",
    "    print(all_files)\n",
    "    \n",
    "    out_dir = Path(output_path)\n",
    "    out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    r = sum(pl.col(col)*coef for col, coef in weights)\n",
    "    iter = 0\n",
    "    for file in tqdm(all_files):\n",
    "        iter += 1\n",
    "        data = pl.scan_parquet(file).with_columns([\n",
    "            r.cast(pl.Float32).alias(\"rating\")\n",
    "        ]).drop(columns).drop(\"last_status\")\n",
    "    \n",
    "        data.sink_parquet(out_dir / f\"{iter}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5915f4da-cbc8-4267-8a47-6891e094b2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_matrix_R(Path(\"./data/interactions\"), Path(\"./data/matrix_R_coord\"), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978c79c-9f78-499a-ae8c-5b8fd7ca8f55",
   "metadata": {},
   "source": [
    "#### 4. Обучаем ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0114b1a5-c313-416f-b697-f76bd716ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_ALS_with_R_matrix(\n",
    "    path_to_R='./data/full_matrix2.npz',\n",
    "    *,\n",
    "    factors=200,\n",
    "    iterations=50,\n",
    "    regularization=0.05\n",
    "):\n",
    "    R = sparse.load_npz(path_to_R)\n",
    "    model = implicit.als.AlternatingLeastSquares(factors, iterations,regularization)\n",
    "    model.fit(R)\n",
    "    model.save(f'./data/als_model_{factors}_{iterations}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc28d5-dc98-4514-9c1c-1bdea4d71454",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_ALS_with_R_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651cdf9e-4f2c-4b30-98d9-f0d72cdf77a8",
   "metadata": {},
   "source": [
    "#### 5. Отбор кандидатов (товаров) для каждого пользователя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c19a80-e9db-4a25-810f-37a7a2ce36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryTree:\n",
    "    def __init__(self):\n",
    "        self.parent = defaultdict(int)\n",
    "        self.children = defaultdict(set)\n",
    "        self.subtree = defaultdict(set)\n",
    "        self.subtree_children = defaultdict(set)\n",
    "\n",
    "    @classmethod\n",
    "    def from_df(cls, category_tree_df: pl.DataFrame):\n",
    "        self = cls()\n",
    "        for path in category_tree_df['ids']:\n",
    "            for i in range(len(path) - 1):\n",
    "                cur_id = int(path[i])\n",
    "                parent_id = path[i + 1]\n",
    "\n",
    "                if cur_id not in self.parent:\n",
    "                    self.parent[cur_id] = parent_id\n",
    "\n",
    "                if parent_id != -1:\n",
    "                    self.children[parent_id].add(cur_id)\n",
    "\n",
    "                self.children.setdefault(cur_id, set())\n",
    "\n",
    "        for p in {v for v in self.parent.values() if v != -1}:\n",
    "            self.children.setdefault(p, set())\n",
    "            self.parent.setdefault(p, -1)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def is_leaf(self, catalog_id: int):\n",
    "        if catalog_id not in self.children:\n",
    "            raise RuntimeError(\"There is no catalog_id in category tree\")\n",
    "        return len(self.children[catalog_id]) == 0\n",
    "\n",
    "    def get_subtree(self, catalog_id: int):\n",
    "        # Use pre-calculated subtree\n",
    "        if catalog_id in self.subtree:\n",
    "            return self.subtree[catalog_id]\n",
    "\n",
    "        subtree = set()\n",
    "        subtree.add(catalog_id)\n",
    "\n",
    "        for son in self.children[catalog_id]:\n",
    "            subtree |= self.get_subtree(son)\n",
    "\n",
    "        self.subtree[catalog_id] = subtree\n",
    "        return subtree\n",
    "\n",
    "    def get_subtree_children(self, catalog_id: int):\n",
    "        # Use pre-calculated subtree\n",
    "        if catalog_id in self.subtree_children:\n",
    "            return self.subtree_children[catalog_id]\n",
    "\n",
    "        subtree_children = set()\n",
    "        if self.is_leaf(catalog_id):\n",
    "            subtree_children.add(catalog_id)\n",
    "\n",
    "        for son in self.children[catalog_id]:\n",
    "            subtree_children |= self.get_subtree_children(son)\n",
    "\n",
    "        self.subtree_children[catalog_id] = subtree_children\n",
    "        return subtree_children\n",
    "\n",
    "    # Leaves only leaves\n",
    "    def find_relevant_categories(self, users_interesting_categories: list, k_nearest=5, n=30):\n",
    "        categories = set(users_interesting_categories)\n",
    "        relevant_categories = set()\n",
    "\n",
    "        for catalog_id in categories:\n",
    "            if catalog_id is None:\n",
    "                continue\n",
    "            \n",
    "            relevant_categories |= self.get_subtree_children(catalog_id)\n",
    "            # if category is a leaf then we can choose k nearest leaves\n",
    "            if self.is_leaf(catalog_id):\n",
    "                parent_id = self.parent.get(catalog_id, -1)\n",
    "                if parent_id != -1:\n",
    "                    # choose k random children of the subtree of the parent without current 'catalog_id'\n",
    "                    siblings = list(self.get_subtree_children(parent_id) - {catalog_id})\n",
    "                    relevant_categories.update(random.sample(siblings, min(k_nearest, len(siblings))))\n",
    "\n",
    "        # Take N random relevant categories\n",
    "        relevant_categories = list(relevant_categories)\n",
    "        return random.sample(relevant_categories, min(n, len(relevant_categories)))\n",
    "\n",
    "    def get_users_relevant_categories(self, users_interacted_categories: pl.DataFrame,\n",
    "                                      interacted_categories_column='interacted_categories',\n",
    "                                      n=30):\n",
    "        return (\n",
    "            users_interacted_categories\n",
    "            .with_columns(\n",
    "                pl.col(interacted_categories_column)\n",
    "                .map_elements(lambda categories: self.find_relevant_categories(categories),\n",
    "                              return_dtype=pl.List(pl.Int64))\n",
    "                .alias('relevant_categories')\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb18133-cc30-41d4-8af8-ea309899a1d4",
   "metadata": {},
   "source": [
    "Создаём дерево категорий и инициализируем его из приложенного датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7464c923-98bf-4ed6-95d0-2aa39a4a3956",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_tree = CategoryTree.from_df(read_df_from_parquets('./data/ml_ozon_recsys_train_final_categories_tree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a1b92-20c9-4e2a-85c7-004f0390892e",
   "metadata": {},
   "source": [
    "Делаем сводную таблицу с релевантными категориями и брендами, с которыми взаимодействовал тот или иной пользователь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe9b435-07b4-407b-afe0-ff8490e9920c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_relevant_categories_and_brands(\n",
    "    path_to_interactions: str,\n",
    "    path_to_items: str,\n",
    "    weights: dict,\n",
    "    category_tree: CategoryTree,\n",
    "    *,\n",
    "    percent_top_items=0.40,\n",
    "    n_top_categories=50,\n",
    "    n_interactions_files=None,\n",
    "    n_items_files=None\n",
    "):\n",
    "    interactions = scan_df_from_parquets(path_to_interactions, n_interactions_files)\n",
    "    items = scan_df_from_parquets(path_to_items, n_items_files)\n",
    "\n",
    "    items_with_brands = items.with_columns(\n",
    "        pl.col(\"attributes\").map_elements(\n",
    "            lambda lst: next(\n",
    "                (x[\"attribute_value\"] for x in lst if x[\"attribute_name\"] == \"Brand\"),\n",
    "                None\n",
    "            ),\n",
    "            return_dtype=pl.Utf8,\n",
    "        ).alias(\"brand\")\n",
    "    )\n",
    "    \n",
    "    interactions_rated_with_brand_and_category = (\n",
    "        interactions\n",
    "        .with_columns(\n",
    "            pl.sum_horizontal([\n",
    "                pl.col(c).cast(pl.Float64).fill_null(0.0) * float(w)\n",
    "                for c, w in weights.items()\n",
    "            ]).alias(\"rating\")\n",
    "        )\n",
    "        .join(\n",
    "            items_with_brands\n",
    "            .select([\n",
    "                \"item_id\",\n",
    "                pl.col(\"catalogid\").alias(\"category\"),\n",
    "                \"brand\"\n",
    "            ]),\n",
    "            on=\"item_id\",\n",
    "            how=\"left\"\n",
    "        )\n",
    "        .select(\"user_id\", \"item_id\", \"category\", \"brand\", \"rating\")\n",
    "    )\n",
    "\n",
    "    # Для каждого пользователя находим самые интересные ему товары (берем долю S наилучших)\n",
    "    top_items_for_users = (\n",
    "        interactions_rated_with_brand_and_category\n",
    "        .sort([\"user_id\", \"rating\"], descending=[False, True])\n",
    "        .with_columns([\n",
    "            pl.len().over(\"user_id\").alias(\"n\"),\n",
    "            pl.arange(0, pl.len()).over(\"user_id\").alias(\"rank\"),\n",
    "        ])\n",
    "        .with_columns(\n",
    "            pl.max_horizontal(pl.lit(1), (pl.col(\"n\") * percent_top_items).ceil().cast(pl.Int64)).alias(\"k\")\n",
    "        )\n",
    "        .filter(pl.col(\"rank\") < pl.col(\"k\"))\n",
    "        .drop(['rank', 'n', 'k'])\n",
    "    )\n",
    "\n",
    "    # Для каждого пользователя берём N наиболее интересных категорий \n",
    "    # Рейтинг категории для пользователя = суммарный рейтинг всех товаров данной категории,\n",
    "    # с которыми взаимодействовал пользователь\n",
    "    top_n_categories = (\n",
    "        top_items_for_users\n",
    "        .select(['user_id', 'category', 'rating'])\n",
    "        .group_by(['user_id', 'category'])\n",
    "        .agg(pl.col('rating').sum().alias(\"summary_rating\"))\n",
    "        .group_by('user_id')\n",
    "        .agg(\n",
    "            pl.col('category')\n",
    "                .sort_by(pl.col('summary_rating'), descending=True)\n",
    "                .head(n_top_categories)\n",
    "                .alias('interacted_categories')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Находим ближайших соседей в дереве\n",
    "    relevant_top_n_categories = category_tree.get_users_relevant_categories(top_n_categories)\n",
    "\n",
    "    # Аналогично находим наиболее интересные бренды для пользователя\n",
    "    # Бренды срезать не будем, просто возьмём наиболее релевантные товары\n",
    "    top_brands = (\n",
    "        top_items_for_users\n",
    "        .filter(pl.col(\"brand\").is_not_null() & (pl.col(\"brand\") != \"Нет бренда\"))\n",
    "        .group_by(\"user_id\")\n",
    "        .agg(pl.col(\"brand\").unique().alias(\"relevant_brands\"))\n",
    "    )\n",
    "\n",
    "    # Собираем в одну таблицу\n",
    "    users_relevant_categories_and_brands = (\n",
    "        relevant_top_n_categories\n",
    "        .join(top_brands, on='user_id', how='left')\n",
    "        .select(['user_id', 'relevant_categories', 'relevant_brands'])\n",
    "    )\n",
    "\n",
    "    return users_relevant_categories_and_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be1584-fb1c-422e-953a-9b2ce97cf3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_relevant_categories_and_brands = get_users_relevant_categories_and_brands(\n",
    "    './data/interactions',\n",
    "    './data/ml_ozon_recsys_train_final_apparel_items_data',\n",
    "    weights,\n",
    "    category_tree\n",
    ").collect(engine='streaming')\n",
    "\n",
    "print(users_relevant_categories_and_brands.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181eb24a-932e-46f1-8809-46ae105648a8",
   "metadata": {},
   "source": [
    "Сохраняем на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be3e6b-d6a6-4338-a52f-8429c7388313",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parquet_in_chunks(users_relevant_categories_and_brands, './data/users_relevant_categories_and_brands')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d7257-74b9-4d21-8485-88f4426887b8",
   "metadata": {},
   "source": [
    "#### 6. Делаем предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be0af9-5dfb-4545-b113-de3466febc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown = joblib.load(\"unknown_ids.joblib\")\n",
    "encoded = joblib.load(\"encoded_ids.joblib\")\n",
    "R = sparse.load_npz(\"./data/full_matrix2.npz\")\n",
    "le_i = joblib.load(\"./models/label_enc_items.joblib\")\n",
    "model = implicit.als.AlternatingLeastSquares()\n",
    "model = model.load(\"./models/als_model_50_20_test.npz\")\n",
    "categories_enc = pl.scan_parquet(\"./data/users_relevant_categories_encoded/*.parquet\").collect()\n",
    "catalog2items = pl.scan_parquet(\"./data/catalog2items.parquet\").collect()\n",
    "catalog2items = (catalog2items.with_columns(pl.Series(\"item_id\", \n",
    "                                                      [le_i.transform(catalog2items[\"item_id\"][i]) for i in range(len(catalog2items))]))\n",
    "                                                                                            .to_pandas().set_index(\"catalogid\"))\n",
    "                                                                             .to_pandas().set_index(\"catalogid\"))\n",
    "catalog2items_dict = catalog2items.to_dict()[\"item_id\"]\n",
    "user2catalogs = (categories_enc.group_by(\"user_id\")\n",
    "                             .agg(pl.col(\"relevant_categories\").unique()))\n",
    "user2catalogs = user2catalogs.to_pandas().set_index(\"user_id\").map(lambda x: x[0])\n",
    "user2catalogs_dict = user2catalogs.to_dict()[\"relevant_categories\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f04eb1-260f-4f47-b00f-03a2a840024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def get_items_for_catalog(catalogid):\n",
    "    # res = catalog2items_dict.get(catalogid)\n",
    "    # return res[:min(len(res), 10_000)]\n",
    "    return catalog2items_dict.get(catalogid)\n",
    "\n",
    "\n",
    "def get_item_candidates_for_user(user_id, R_row):\n",
    "    catalogs =  user2catalogs_dict[user_id]\n",
    "    catalogs = catalogs[:min(len(catalogs),30)]\n",
    "    \n",
    "    candidates = np.concatenate([catalog2items_dict.get(c, np.empty(0, dtype=np.int32)) for c in catalogs])\n",
    "    candidates = np.unique(candidates) \n",
    "\n",
    "\n",
    "    return np.setdiff1d(candidates, R_row.indices, assume_unique=True)\n",
    "\n",
    "\n",
    "def make_prediction(user_emb:np.array, item_candidates, item_candidates_emb:np.array):\n",
    "    scores = item_candidates_emb @ user_emb  \n",
    "    top_idx = np.argpartition(scores, -100)[-100:]\n",
    "    top_idx = top_idx[scores[top_idx].argsort()[::-1]]\n",
    "\n",
    "    return item_candidates[top_idx]\n",
    "\n",
    "\n",
    "def predict_batch(R_trunc, batch_user_ids):\n",
    "    results = {}\n",
    "    item_factors = model.item_factors\n",
    "    user_factors = model.user_factors\n",
    "    for i in range(len(batch_user_ids)):\n",
    "        user = batch_user_ids[i]\n",
    "        candidates = get_item_candidates_for_user(user, R_trunc[i])\n",
    "\n",
    "        item_candidates_emb = item_factors[candidates]\n",
    "        res = make_prediction(user_factors[user], candidates, item_candidates_emb)\n",
    "        results[user] = res\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b47619-5b01-45bf-8ea6-db7009e3e006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "def chunks(lst, batch_size):\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i + batch_size]\n",
    "\n",
    "\n",
    "batch_size = 300    \n",
    "all_results = {}\n",
    "print(\"total iters = \", 470000 / batch_size)\n",
    "\n",
    "# for batch_users in tqdm(chunks(encoded, batch_size)):\n",
    "    # all_results.update(predict_batch2( R[batch_users], batch_users,  le_i, None))\n",
    "iterations = 0\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    futures = []\n",
    "    for batch_users in tqdm(chunks(encoded, batch_size)):\n",
    "        futures.append(\n",
    "            executor.submit(\n",
    "                predict_batch, R[batch_users], batch_users\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for future in tqdm(as_completed(futures)):\n",
    "        iterations += 1\n",
    "        all_results.update(future.result())\n",
    "        # if iterations % 100 == 0:\n",
    "        #     df = pd.DataFrame({\n",
    "        #         'values': list(all_results.values())\n",
    "        #     }, index=list(all_results.keys()))\n",
    "        #     df.to_csv(f'output_2_{iterations}.csv')\n",
    "\n",
    "# with ProcessPoolExecutor(max_workers=6) as executor:\n",
    "#     futures = {executor.submit(predict_batch, R[batch], batch, le_i, None): batch\n",
    "#                for batch in tqdm(chunks(encoded, batch_size))}\n",
    "\n",
    "#     for i, future in tqdm(enumerate(as_completed(futures), 1)):\n",
    "#         all_results.update(future.result())\n",
    "#         if i % 300 == 0:\n",
    "#             pd.DataFrame.from_dict(all_results, orient=\"index\").to_csv(f\"output2_{i}.csv\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'values': list(all_results.values())\n",
    "}, index=list(all_results.keys()))\n",
    "df.to_csv('output3.csv')\n",
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96a743d-ebad-4457-8f2f-59e60e2848c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_decoder = joblib.load(\"models/ordinal_enc_users.joblib\")\n",
    "item_decoder = joblib.load(\"models/label_enc_items.joblib\")\n",
    "\n",
    "def decode_items(encoded_list):\n",
    "        return item_decoder.inverse_transform(encoded_list)\n",
    "\n",
    "def decode_and_merge(pred_df: pd.DataFrame, input_file: str) -> dict:\n",
    "    # user_ids = user_decoder.categories_[0]\n",
    "    decoded_users = user_decoder.inverse_transform(pred_df.index.to_numpy().reshape(-1,1)).ravel()\n",
    "    # print(decoded_users)\n",
    "    values = np.concatenate(pred_df[\"values\"].to_numpy())\n",
    "    # display(values)\n",
    "    decoded = item_decoder.inverse_transform(values).reshape(-1,100)\n",
    "    # print(decoded.shape)\n",
    "    \n",
    "    df_decoded = pd.DataFrame({\"user_id\": decoded_users,\n",
    "                            \"values\": [row for row in decoded],\n",
    "                               })\n",
    "    \n",
    "    extra_df = pd.read_csv(input_file, header=None)\n",
    "    extra_df[1] =  extra_df[1].map(lambda x: np.array(list(map(int, x[1:-1].split()))))\n",
    "    extra_df.columns = [\"user_id\", \"values\"]\n",
    "    # merged_df = extra_df.merge(df_decoded, on=\"user_id\", how=\"inner\")\n",
    "    merged_df = pd.concat([df_decoded, extra_df])\n",
    "    # display(merged_df)\n",
    "\n",
    "    \n",
    "    result_dict = dict(zip(merged_df[\"user_id\"], merged_df[\"values\"]))\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "decoded = decode_and_merge(df, 'unknown_users.csv')\n",
    "\n",
    "def create_submission(recommendations, filename='submission5.csv'):\n",
    "\n",
    "    print(f\"Создаем файл submission: {filename}\")\n",
    "    \n",
    "    submission_data = []\n",
    "    for user_id, items in tqdm(recommendations.items(), desc=\"Формирование submission\"):\n",
    "        submission_data.append({\n",
    "            'user_id': user_id,\n",
    "            'item_id_1 item_id_2 ... item_id_100': ' '.join(map(str, items))\n",
    "        })\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_data)\n",
    "    submission_df.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"Сохранен submission с {len(submission_df):,} пользователями\")\n",
    "    print(f\"Размер файла: {Path(filename).stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "submission_file = create_submission(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
