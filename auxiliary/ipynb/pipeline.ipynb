{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a401266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from tqdm import tqdm \n",
    "import math \n",
    "import glob\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ef7c02",
   "metadata": {},
   "source": [
    "### 1. Обработка таблицы логов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f829222",
   "metadata": {},
   "source": [
    "Сначала внутри каждого .parquet файла происходит агрегация и создается сгрупированный файл .parquet в папку output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38764feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути\n",
    "orders_path = \"./data/ml_ozon_recsys_train_final_apparel_orders_data/*.parquet\"\n",
    "tracker_path = \"./data/ml_ozon_recsys_train_final_apparel_tracker_data/*.parquet\"\n",
    "output_dir = Path(\"./data/train_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "df_orders = (pl.scan_parquet(orders_path)\n",
    "             .select([\"user_id\", \"item_id\", \"last_status\"])\n",
    "             .with_columns([\n",
    "                   pl.col(\"user_id\").cast(pl.Int64),\n",
    "                    pl.col(\"item_id\").cast(pl.Int64),\n",
    "                    pl.col(\"last_status\")\n",
    "             ])\n",
    "             .filter(pl.col(\"last_status\") != \"proccesed_orders\"))\n",
    "\n",
    "\n",
    "tracker_files = sorted(glob.glob(tracker_path))\n",
    "chunk_size = 1  \n",
    "\n",
    "def encode_actions(df):\n",
    "    return df.with_columns([\n",
    "        (pl.col(\"action_type\") == \"view_description\").cast(pl.Int32).alias(\"action_type_view_description\"),\n",
    "        (pl.col(\"action_type\") == \"to_cart\").cast(pl.Int32).alias(\"action_type_to_cart\"),\n",
    "        (pl.col(\"action_type\") == \"page_view\").cast(pl.Int32).alias(\"action_type_page_view\"),\n",
    "        (pl.col(\"action_type\") == \"favorite\").cast(pl.Int32).alias(\"action_type_favorite\"),\n",
    "        (pl.col(\"action_type\") == \"unfavorite\").cast(pl.Int32).alias(\"action_type_unfavorite\"),\n",
    "        (pl.col(\"action_type\") == \"review_view\").cast(pl.Int32).alias(\"action_type_review_view\"),\n",
    "        (pl.col(\"action_type\") == \"remove\").cast(pl.Int32).alias(\"action_type_remove\"),\n",
    "        pl.when(pl.col(\"last_status\") == \"canceled_orders\").then(0)\n",
    "          .when(pl.col(\"last_status\") == \"delivered_orders\").then(1)\n",
    "          .otherwise(0)\n",
    "          .cast(pl.Int8)\n",
    "          .alias(\"last_status\")\n",
    "    ]).drop(\"action_type\")\n",
    "\n",
    "for i in range(0, len(tracker_files), chunk_size):\n",
    "    files_chunk = tracker_files[i:i+chunk_size]\n",
    "    print(f\"Processing chunk {i//chunk_size + 1}/{(len(tracker_files)+chunk_size-1)//chunk_size}\")\n",
    "\n",
    "    chunk_df = (pl.scan_parquet(files_chunk)\n",
    "                .with_columns([\n",
    "                    pl.col(\"user_id\").cast(pl.Int64),\n",
    "                    pl.col(\"item_id\").cast(pl.Int64),\n",
    "                ]))\n",
    "\n",
    "    temp = (\n",
    "        chunk_df\n",
    "        .join(df_orders, on=[\"item_id\",\"user_id\"], how=\"left\")\n",
    "    )\n",
    "    temp = encode_actions(temp)\n",
    "\n",
    "    temp_agg = temp.group_by([\"user_id\",\"item_id\"]).agg([\n",
    "        pl.col([\n",
    "            \"action_type_view_description\",\n",
    "            \"action_type_to_cart\",\n",
    "            \"action_type_page_view\",\n",
    "            \"action_type_favorite\",\n",
    "            \"action_type_unfavorite\",\n",
    "            \"action_type_review_view\",\n",
    "            \"action_type_remove\"\n",
    "        ]).sum(),\n",
    "        pl.col(\"last_status\").first()\n",
    "    ])\n",
    "\n",
    "    output_file = output_dir / f\"agg_chunk_{i//chunk_size}.parquet\"\n",
    "    temp_agg.collect(streaming=True).write_parquet(output_file)\n",
    "    print(f\"Saved {output_file}\")\n",
    "\n",
    "\n",
    "all_chunks = pl.scan_parquet(str(output_dir / \"*.parquet\"))\n",
    "\n",
    "final_result = (\n",
    "    all_chunks\n",
    "    .group_by([\"user_id\",\"item_id\"])\n",
    "    .agg([\n",
    "        pl.col([\n",
    "            \"action_type_view_description\",\n",
    "            \"action_type_to_cart\",\n",
    "            \"action_type_page_view\",\n",
    "            \"action_type_favorite\",\n",
    "            \"action_type_unfavorite\",\n",
    "            \"action_type_review_view\",\n",
    "            \"action_type_remove\"\n",
    "        ]).sum(),\n",
    "        pl.col(\"last_status\").first()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a359054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_result, temp_agg, files_chunk,chunk_df, df_orders, all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d644a",
   "metadata": {},
   "source": [
    "Потом проводим сжатие нескольких parquet файлов в один"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ebdc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_dir = Path(\"./data/grouped3\")\n",
    "\n",
    "all_files = sorted(glob.glob(str(chunks_dir / \"*.parquet\")))\n",
    "\n",
    "\n",
    "out_path = Path(\"./data/grouped4\")\n",
    "out_path.mkdir(exist_ok=True)\n",
    "n = 2\n",
    "\n",
    "for i in tqdm(range(0, len(all_files), n)):\n",
    "    batch = all_files[i:i+n]\n",
    "    \n",
    "    # читаем сразу все parquet из батча\n",
    "    df = pl.scan_parquet(batch)\n",
    "    agg_data = (\n",
    "    df\n",
    "    .group_by([\"user_id\",\"item_id\"])\n",
    "    .agg([\n",
    "        pl.col([\n",
    "            \"action_type_view_description\",\n",
    "            \"action_type_to_cart\",\n",
    "            \"action_type_page_view\",\n",
    "            \"action_type_favorite\",\n",
    "            \"action_type_unfavorite\",\n",
    "            \"action_type_review_view\",\n",
    "            \"action_type_remove\"\n",
    "        ]).sum(),\n",
    "        pl.col(\"last_status\").first()\n",
    "    ])\n",
    "    )\n",
    "    del df\n",
    "    out_file = out_path / f\"grouped_batch_{i//n + 1}.parquet\"\n",
    "    agg_data.sink_parquet(out_file)\n",
    "    del agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1a71b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
